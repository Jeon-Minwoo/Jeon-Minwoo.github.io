---
layout: post
title: Attention & Transformer
date: 2022-07-28 19:20:23 +0900
category: Transformer
---
# Attention & Transformer

### Contents

1. NLPì—ì„œ attention ë“±ì¥ ë°°ê²½
2. NLP(seq2seq)ì—ì„œ Attention
3. Transformer (Attention is all you need)

## NLPì—ì„œ attention ë“±ì¥ ë°°ê²½

**seq2seq(Sequence to Sequence) structure ë¬¸ì œì **

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled.png)

- Encoderì—ì„œ ê° ë‹¨ì–´ì˜ embeddingê³¼ RNNì˜ hidden stateë¥¼ ê±°ì³ì„œ ì •ë³´ê°€ ì••ì¶•ì´ ë˜ê³ , Encoderì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì˜ ì¶œë ¥ì´ context vectorê°€ ëœë‹¤.
- Decoderì—ì„œëŠ” context vectorì™€ ë¬¸ì¥ì˜ ì²˜ìŒì„ í‘œì‹œí•˜ëŠ” SOS(Start of Sequence) ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤.
- ê° ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ì¸ Context Vectorì•ˆì— í•¨ì¶•ì‹œí‚¤ëŠ” ë° ìˆì–´ ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•œë‹¤.
â†’ Gradient Vanishing ë°œìƒ
â†’ Input Dataê°€ Sequentialí•˜ê²Œ ë“¤ì–´ê°„ë‹¤.

**Improved seq2seq**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%201.png)

- ì—¬ëŸ¬ ê°œì˜ ë‹¨ì–´ embeddingì„ context vectorì— í•¨ì¶•í•  í•„ìš”ê°€ ì—†ë‹¤.
- ì´ì „ì— ì…ë ¥ëœ ë°ì´í„°ì— ëŒ€í•´ì„œ ì¶œë ¥ì˜ ì˜í–¥ì´ ë‚®ì•„ì§€ëŠ” ë¬¸ì œê°€ ì™„í™”ëœë‹¤.
- gradient vanishingì´ ì™„í™”ëœë‹¤.
- Encoderì˜ hidden stateë¥¼ ëª¨ë‘ ë„˜ê²¨ì£¼ë©´ ì…ë ¥ì˜ ì°¨ì›ì´ ì»¤ì§€ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤.

## NLP(seq2seq)ì—ì„œ Attention

**Attention Mechanism**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%202.png)

Attentionì˜ 3 ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- Query: ì§ˆì˜, ì°¾ê³ ì í•˜ëŠ” ëŒ€ìƒ.
- Key: í‚¤, ì €ì¥ëœ ë°ì´í„°ë¥¼ ì°¾ê³ ì í•  ë•Œ ì°¸ì¡°í•˜ëŠ” ê°’
- Value: ê°’, ì €ì¥ë˜ëŠ” ë°ì´í„°
- Attentionì—ì„œëŠ” Queryì— ëŒ€í•´ì„œ ì–´ë–¤ Keyì™€ ìœ ì‚¬í•œì§€ ë¹„êµë¥¼ í•˜ê³ , ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•˜ì—¬ Keyì— ëŒ€ì‘í•˜ëŠ” Valueë¥¼ í•©ì„±(Aggregation)í•œ ê²ƒì´ Attention Valueê°€ ëœë‹¤.

**Applying attention to seq2seq**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%203.png)

- QueryëŠ” Decoderì˜ hidden stateê°€ ëœë‹¤.
- Key, ValueëŠ” Encoderì˜ hidden stateê°€ ëœë‹¤.
- ë¨¼ì € Decoderì˜ hidden stateëŠ” RNN(LSTM)ì—ì„œ ë°›ì•„ì„œ ì—°ì‚°í•˜ì—¬Â $s_i$Â â†’Â $s_{i+1}$ë¡œ ë§Œë“­ë‹ˆë‹¤.
- ê·¸ í›„ Attenen valueì¸Â $a_i$ì™€Â $s_{i+1}$ì„ concat ($v_i=[s_i;a_{i-1}]$) í•˜ì—¬Â ì„ ë§Œë“­ë‹ˆë‹¤. ì´ ê°’ì„ FC layerì™€ Softmaxë¥¼ ê±°ì³ì„œ ìµœì¢… ì¶œë ¥ì¸Â $y_i$ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
- ìœ„ì˜ êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ì´ë„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
    
    ![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%204.png)
    

## Transformer (Attention is all you need)

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%205.png)

**Positional Encoding**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%206.png)

- RNNì—ì„œëŠ” ëª¨ë¸ ìì²´ì ìœ¼ë¡œ inputì˜ ìˆœì„œë¥¼ ê³ ë ¤í•œë‹¤.
- Recurrent connectionì´ ì œê±°ëœ Self-attentionì—ì„œëŠ” ëª¨ë¸ ìì²´ê°€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì´ìš©í•  ìˆ˜ ì—†ë‹¤.
- ì…ë ¥ì˜ ìˆœì„œë¥¼ ë°”ê¿”ë„ attention ê²°ê³¼ê°€ ë°”ë€Œì§€ ì•ŠëŠ”ë‹¤. (ìœ„ì˜ ê·¸ë¦¼ ì°¸ê³ )
- Transformerì—ì„œëŠ” ìœ„ì¹˜ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ” positional encoding ì´ìš©

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%207.png)

- TransformerëŠ” RNNê³¼ ë‹¤ë¥´ê²Œ ë³‘ë ¬ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í˜„ì¬ ê³„ì‚°í•˜ê³  ìˆëŠ” ë‹¨ì–´ê°€ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ” ë‹¨ì–´ì¸ì§€ í‘œí˜„í•´ì•¼ í•œë‹¤. â†’ Positional Encoding

**Dot-Product of Positional Encoding**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%208.png)

- Positional Encoding ê°’ì„ ì„œë¡œ Dot-Product ì—°ì‚°ì„ ê±°ì¹˜ë©´ wì™€ të¡œë§Œ ì´ë£¨ì–´ì§„ ê°„ë‹¨í•œ ì‹ì„ ë„ì¶œí•  ìˆ˜ ìˆë‹¤.

**Self-Attention**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%209.png)

- ì¼ë°˜ì ì¸ Attention based modelì—ì„œì˜ ë¹„êµëŒ€ìƒì´ target sequenceì¸ë° ë°˜í•´ self-attentionì—ì„œëŠ” inputê°„ì˜ ê´€ê³„ë¥¼ ë¹„êµí•œë‹¤.

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%2010.png)

- RNNì˜ í•œê³„ì ì„ ì œê±°í•˜ê¸° ìœ„í•´ recurrent connectionì„ ì œê±°í•œë‹¤.
- ì…ë ¥ Sequenceì˜ vectorê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•œë‹¤.

**Scaled Dot-product Attention**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%2011.png)

- Layer normalizationì„ ìˆ˜í–‰í•˜ê³  í›„ vectorëŠ” ğ‘(0,1)ì˜ ë¶„í¬
- Dot productì˜ ê°’ì€ inputì˜ ê°’ì— ë”°ë¼ ë§¤ìš° ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§ˆ ìˆ˜ ìˆë‹¤.
- ì´ë¥¼ ë°©ì§€í•˜ê¸°ìœ„í•´ softmaxë¥¼ ì·¨í•˜ê¸°ì „ ì¼ì •í•œ scalarê°’ìœ¼ë¡œ attention scoreë¥¼ ë‚˜ëˆˆë‹¤.
    
    (ë…¼ë¬¸ì—ì„œëŠ” queriesì™€ keysì˜ dimension ğ‘‘ğ‘˜ì˜ ì œê³±ê·¼ì„ ì´ ê°’ìœ¼ë¡œ ì‚¬ìš©)
    

**Masked Attention**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%2012.png)

[Problem of Self-Attention in Decoder]

- Language modelingì—ì„œ Inferenceí•˜ëŠ” ê³¼ì •ì˜ ë¯¸ë˜ì˜ ì¶œë ¥ì€ ì•Œ ìˆ˜ ì—†ë‹¤.
- ì¼ë°˜ì ì¸ self-attention ê³„ì‚°ë°©ì‹ìœ¼ë¡œëŠ” self-attention scoreë¥¼ ê³„ì‚° í•˜ëŠ” ê³¼ì •ì—ì„œ ë¯¸ë˜ì˜ ì¶œë ¥ë“¤ê¹Œì§€ ê³ ë ¤í•˜ì—¬ attention scoreë¥¼ ê³„ì‚°í•œë‹¤.
- në²ˆì§¸ ì…ë ¥ì— ëŒ€í•œ ì¶œë ¥ì€ n-1ê¹Œì§€ì˜ ì…ë ¥ë§Œì„ ê³ ë ¤í•´ì•¼í•œë‹¤.

â†’ decoderì—ì„œ self attentionì€ ê°ê°ì˜ ì¶œë ¥ë‹¨ì–´ê°€ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ë¥¼ ì „ë¶€ ì°¸ê³ í•˜ì§€ ì•Šê³ , ì•ìª½ì— ë“±ì¥í•œ ë‹¨ì–´ë§Œ ì°¸ê³ í•˜ë„ë¡ Masked attentionì„ ì·¨í•œë‹¤.

**Multi-Head Attention**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%2013.png)

- Scaled Dot-Product Attentionì„ hê°œ ëª¨ì•„ì„œ Attention Layerë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%2014.png)

- ì°¨ì›ì„ ì¶•ì†Œí•´ì„œ ë³´ë˜ ë³‘ë ¬ì ìœ¼ë¡œ ì „ì²´ë¥¼ ê²€í† í•œë‹¤.
- ë³‘ë ¬ ì—°ì‚°ìœ¼ë¡œ ì—°ì‚° ì†ë„ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ì„œ ë‹¤ë°©ë©´ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥í•˜ë‹¤.

**Feed Forward Network**

![Untitled](/public/img/attention_transformer/Transformer%20d14d3d6dad1b49e5a17ca695ee00c09b/Untitled%2015.png)

- Self-Attention Layerì˜ ì¶œë ¥ì—ëŠ” non-linearityê°€ ì ìš©ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ FFN ì¶”ê°€í•œë‹¤.
- ì…ë ¥ì— ëŒ€í•´ì„œ ê°™ì€ weightë¥¼ ê°–ëŠ” FFNì„ positionë³„ë¡œ ì ìš©í•œë‹¤.
- ë…¼ë¬¸ì—ì„œëŠ” ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ = 512, ğ‘‘ğ‘“ğ‘“ = 2048
